# LLM Query Parameter Explorer

Программа для отправки запросов к LLM через OpenAI-совместимый API с перебором всех комбинаций опциональных параметров.

## Структура проекта

```
day-02-2/
├── venv/                 # виртуальное окружение
├── main.py               # основная программа
├── config.ini            # конфигурация (не входит в репозиторий)
├── config.example.ini    # шаблон конфигурации
├── requirements.txt      # зависимости Python
└── README.md             # этот файл
```

## Установка

1. Создайте виртуальное окружение и установите зависимости:

```bash
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

2. Скопируйте шаблон конфигурации:

```bash
cp config.example.ini config.ini
```

3. Отредактируйте `config.ini`, указав ваши данные:

```ini
[openai]
api_key = ваш_api_ключ
base_url = https://openrouter.ai/api/v1
model = deepseek/deepseek-v3.2

[request]
prompt_template = Ваш запрос к модели
response_format = json
max_tokens = 100
stop_sequence = Fight Club
```

## Запуск

```bash
source venv/bin/activate
python main.py
```

## Как это работает

Программа перебирает все 8 комбинаций (2³) трёх опциональных параметров:
- **format** — формат ответа (например, JSON)
- **max_tokens** — ограничение на длину ответа
- **stop** — последовательность завершения генерации

Для каждой комбинации формируется запрос к API и выводится ответ модели.

## Настройка параметров

| Параметр | Секция | Описание |
|----------|--------|----------|
| `api_key` | `[openai]` | API-ключ для доступа к LLM |
| `base_url` | `[openai]` | Базовый URL API (опционально) |
| `model` | `[openai]` | Имя модели для запросов |
| `prompt_template` | `[request]` | Текст запроса к модели |
| `response_format` | `[request]` | Желаемый формат ответа |
| `max_tokens` | `[request]` | Максимальное количество токенов |
| `stop_sequence` | `[request]` | Последовательность для остановки генерации |

## Примечание

> Иногда модель может завершить ответ раньше ожидаемого. Особенно если в запросе есть **определённые правила**, а последовательность остановки содержит **ключевые слова из названия**. В таких случаях первое правило работает безотказно — даже если его явно не формулировали.

Это особенность работы stop-последовательностей: генерация обрывается в момент совпадения, и читатель остаётся в приятном неведении относительно полного списка.
